{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82Ru8RwxFXx",
        "outputId": "13f07099-886c-480c-a5dc-89498e0c29f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijUshx1FxIMG",
        "outputId": "24fa6a52-bda9-46cc-8d56-0efa9fd57371"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.24.52)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.52 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.27.52)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.52->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.52->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.52->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "UuwIhHVhxlGW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JaEjCfoqxayu",
        "outputId": "1e850bad-ec24-4d4d-b971-03cf7d3a3ec6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Import appropriate libraries\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "#Define Access Keys\n",
        "\n",
        "api_key = 't7U9rmjmbvqZCQnherHkrDZ9q'\n",
        "api_key_secret = 'k5FwX8KGxruNp9oUlhNCb1xHtJ1JjIIlI7ozai5vGaMM2pIaCU'\n",
        "\n",
        "access_token = '1532526934269239303-nQpMFHharyoQCfc8FoNuUwAUFdJM6Y'\n",
        "access_token_secret = 'ndON5Ktl0MEOFDtjGMjGVkuRz4D6FWpaPruh2IHUWEgC0'\n",
        "\n",
        "# authentication\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ],
      "metadata": {
        "id": "ACkRQumGx96Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV9bFv-zyiE5",
        "outputId": "07e022ba-b11e-490d-be78-f46930f046f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import corpus csv\n",
        "\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Arkenews/Corpus_Final.csv\")\n",
        "\n",
        "print(df_final.info()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfnD41KByiM1",
        "outputId": "d0ce72a4-4ea7-490f-cbf0-4e6ecb43685a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120218 entries, 0 to 120217\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   User     120217 non-null  object\n",
            " 1   Tweet    120218 non-null  object\n",
            " 2   Label    120218 non-null  object\n",
            " 3   Title    120218 non-null  object\n",
            " 4   Country  120218 non-null  object\n",
            "dtypes: object(5)\n",
            "memory usage: 4.6+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "def cleanUpTweet(txt):\n",
        "    # Remove mentions\n",
        "    txt = re.sub(r'@+', '', txt)\n",
        "    # Remove hashtags\n",
        "    txt = re.sub(r'#[A-Z0-9]+', '', txt)\n",
        "    # Remove retweets:\n",
        "    txt = re.sub(r'RT : ', '', txt)\n",
        "    # Remove urls\n",
        "    txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', txt)\n",
        "    #remove amp\n",
        "    txt = re.sub(r'&amp;', '', txt)\n",
        "    #rempve strange characters\n",
        "    txt = re.sub(r'ðŸ™', '', txt)\n",
        "    #remove new lines\n",
        "    txt = re.sub(r'\\n', ' ', txt)\n",
        "    return txt\n",
        "\n",
        "df_final['Cleaned_Tweet'] = df_final['Tweet'].apply(cleanUpTweet)"
      ],
      "metadata": {
        "id": "vCBddsqeyiPr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode Label: D = 1 / R = 0\n",
        "\n",
        "df_final['Label_Number'] = df_final.Label.map({'R':0, 'D':1})\n",
        "df_final.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rDBlPax7yiSk",
        "outputId": "c3712a52-a44f-4e07-f04a-4eede71880f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       User                                              Tweet Label Title  \\\n",
              "0  RepAdams  I didn't ask for a pardon.\\n\\nBut a certain Pr...     D   HOR   \n",
              "1  RepAdams  Your daily reminder that instead of saying som...     D   HOR   \n",
              "2  RepAdams  I’m proud to work with my Congressional collea...     D   HOR   \n",
              "3  RepAdams  RT @grantstern: Multiple Republican Congressme...     D   HOR   \n",
              "4  RepAdams  RT @MacFarlaneNews: Approx. 140 police officer...     D   HOR   \n",
              "\n",
              "  Country                                      Cleaned_Tweet  Label_Number  \n",
              "0      US  I didn't ask for a pardon.  But a certain Pres...             1  \n",
              "1      US  Your daily reminder that instead of saying som...             1  \n",
              "2      US  I’m proud to work with my Congressional collea...             1  \n",
              "3      US  RT grantstern: Multiple Republican Congressmen...             1  \n",
              "4      US  RT MacFarlaneNews: Approx. 140 police officers...             1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef505763-7a3f-4d96-a348-3142c71fc2d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Label</th>\n",
              "      <th>Title</th>\n",
              "      <th>Country</th>\n",
              "      <th>Cleaned_Tweet</th>\n",
              "      <th>Label_Number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RepAdams</td>\n",
              "      <td>I didn't ask for a pardon.\\n\\nBut a certain Pr...</td>\n",
              "      <td>D</td>\n",
              "      <td>HOR</td>\n",
              "      <td>US</td>\n",
              "      <td>I didn't ask for a pardon.  But a certain Pres...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RepAdams</td>\n",
              "      <td>Your daily reminder that instead of saying som...</td>\n",
              "      <td>D</td>\n",
              "      <td>HOR</td>\n",
              "      <td>US</td>\n",
              "      <td>Your daily reminder that instead of saying som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RepAdams</td>\n",
              "      <td>I’m proud to work with my Congressional collea...</td>\n",
              "      <td>D</td>\n",
              "      <td>HOR</td>\n",
              "      <td>US</td>\n",
              "      <td>I’m proud to work with my Congressional collea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RepAdams</td>\n",
              "      <td>RT @grantstern: Multiple Republican Congressme...</td>\n",
              "      <td>D</td>\n",
              "      <td>HOR</td>\n",
              "      <td>US</td>\n",
              "      <td>RT grantstern: Multiple Republican Congressmen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RepAdams</td>\n",
              "      <td>RT @MacFarlaneNews: Approx. 140 police officer...</td>\n",
              "      <td>D</td>\n",
              "      <td>HOR</td>\n",
              "      <td>US</td>\n",
              "      <td>RT MacFarlaneNews: Approx. 140 police officers...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef505763-7a3f-4d96-a348-3142c71fc2d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef505763-7a3f-4d96-a348-3142c71fc2d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef505763-7a3f-4d96-a348-3142c71fc2d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zC79-SM1yiVV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_final"
      ],
      "metadata": {
        "id": "notI26NqzPXP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.Cleaned_Tweet.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.Label_Number.values"
      ],
      "metadata": {
        "id": "4y7-oF9kzAFP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcWYPq-gzIZR",
        "outputId": "def8e82c-2a4a-40d3-e305-1cdc7b8ddb4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1244128.51B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'i', 'didn', \"'\", 't', 'ask', 'for', 'a', 'pardon', '.', 'but', 'a', 'certain', 'president', 'who', '\"', 'engaged', 'in', 'insurrection', 'or', 'rebellion', 'against', 'the', '[', 'united', 'states', ']', ',', 'or', 'given', 'aid', 'or', 'comfort', 'to', 'the', 'enemies', 'thereof', '\"', 'may', 'need', 'to', 'ask', 'both', 'houses', 'of', 'congress', 'to', 'restore', 'their', 'ability', 'to', 'hold', 'office', ',', 'per', 'the', '14th', 'amendment', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "metadata": {
        "id": "h1Q5VUt9zx2R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "metadata": {
        "id": "j6jvo3yYz5Pw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "a2nyyoWwz5Yb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "metadata": {
        "id": "gmq7LcpFz5bi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "metadata": {
        "id": "9preR5TRz5eJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "id": "qZQTTWPZz5gz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "1sy2Ywmaz5j0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzKcei-Wz5nQ",
        "outputId": "bba7f297-d8d3-4709-87b4-f2f2c3d57499"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 36394368.41B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "metadata": {
        "id": "0rF1Gcd50UrV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bccwRIBN0ZQE",
        "outputId": "395398f0-17e3-4153-bdd3-bd89cf53011e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "QfzVULLM0bVB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = [] \n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZCYSpNU0hLQ",
        "outputId": "0d80cf4f-21da-4822-9f2d-1fe4330199a7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.3362326882305951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [27:14<1:21:43, 1634.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8837720623791102\n",
            "Train loss: 0.18513902582201747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [54:26<54:26, 1633.12s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8875120889748549\n",
            "Train loss: 0.10800431850802766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [1:21:39<27:13, 1633.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8885094294003868\n",
            "Train loss: 0.06326998613065098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 4/4 [1:48:54<00:00, 1633.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8906703336557059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "49OrLR3K0kHw",
        "outputId": "f1531a96-65b4-4870-9edb-399e3ab837c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " train = df_final\n",
        "test =  df_final"
      ],
      "metadata": {
        "id": "YpO-dtxW05JO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=test"
      ],
      "metadata": {
        "id": "dRuuv06b05QC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.Cleaned_Tweet.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.Label_Number.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "5XyXbqwp05UP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "metadata": {
        "id": "Za-LaHv-05Ww"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  matthews = matthews_corrcoef(true_labels[i],\n",
        "                 np.argmax(predictions[i], axis=1).flatten())\n",
        "  matthews_set.append(matthews)"
      ],
      "metadata": {
        "id": "v4U3pVXY05Zm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matthews_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6WDKGOe05cj",
        "outputId": "2c4439c6-c39f-4bd5-b2ce-b4b302799494"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.936441710371274,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.8783100656536799,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.8509629433967631,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.8805899139163632,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.8027729719194864,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n"
      ],
      "metadata": {
        "id": "TTNgrimr1kor"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MbOo-Nm1lPH",
        "outputId": "b0e367b6-7e4b-4e81-8ee4-b2e19aaad813"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.962448081684414"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-cCX4uuM1nl7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}